{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо предсказать биологический ответ молекул (столбец 'Activity') по их химическому составу (столбцы D1-D1776)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#импорт библиотек\n",
    "import numpy as np #для матричных вычислений\n",
    "import pandas as pd #для анализа и предобработки данных\n",
    "import matplotlib.pyplot as plt #для визуализации\n",
    "import seaborn as sns #для визуализации\n",
    "\n",
    "from sklearn import linear_model #линейные моделиё\n",
    "from sklearn import tree #деревья решений\n",
    "from sklearn import ensemble #ансамбли\n",
    "from sklearn import metrics #метрики\n",
    "from sklearn import preprocessing #предобработка\n",
    "from sklearn.model_selection import train_test_split #сплитование выборки\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Activity</th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "      <th>D6</th>\n",
       "      <th>D7</th>\n",
       "      <th>D8</th>\n",
       "      <th>D9</th>\n",
       "      <th>...</th>\n",
       "      <th>D1767</th>\n",
       "      <th>D1768</th>\n",
       "      <th>D1769</th>\n",
       "      <th>D1770</th>\n",
       "      <th>D1771</th>\n",
       "      <th>D1772</th>\n",
       "      <th>D1773</th>\n",
       "      <th>D1774</th>\n",
       "      <th>D1775</th>\n",
       "      <th>D1776</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.497009</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.132956</td>\n",
       "      <td>0.678031</td>\n",
       "      <td>0.273166</td>\n",
       "      <td>0.585445</td>\n",
       "      <td>0.743663</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.606291</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111209</td>\n",
       "      <td>0.803455</td>\n",
       "      <td>0.106105</td>\n",
       "      <td>0.411754</td>\n",
       "      <td>0.836582</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>0.480124</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.209791</td>\n",
       "      <td>0.610350</td>\n",
       "      <td>0.356453</td>\n",
       "      <td>0.517720</td>\n",
       "      <td>0.679051</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538825</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.196344</td>\n",
       "      <td>0.724230</td>\n",
       "      <td>0.235606</td>\n",
       "      <td>0.288764</td>\n",
       "      <td>0.805110</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.517794</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.494734</td>\n",
       "      <td>0.781422</td>\n",
       "      <td>0.154361</td>\n",
       "      <td>0.303809</td>\n",
       "      <td>0.812646</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1777 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Activity        D1        D2    D3   D4        D5        D6        D7  \\\n",
       "0         1  0.000000  0.497009  0.10  0.0  0.132956  0.678031  0.273166   \n",
       "1         1  0.366667  0.606291  0.05  0.0  0.111209  0.803455  0.106105   \n",
       "2         1  0.033300  0.480124  0.00  0.0  0.209791  0.610350  0.356453   \n",
       "3         1  0.000000  0.538825  0.00  0.5  0.196344  0.724230  0.235606   \n",
       "4         0  0.100000  0.517794  0.00  0.0  0.494734  0.781422  0.154361   \n",
       "\n",
       "         D8        D9  ...  D1767  D1768  D1769  D1770  D1771  D1772  D1773  \\\n",
       "0  0.585445  0.743663  ...      0      0      0      0      0      0      0   \n",
       "1  0.411754  0.836582  ...      1      1      1      1      0      1      0   \n",
       "2  0.517720  0.679051  ...      0      0      0      0      0      0      0   \n",
       "3  0.288764  0.805110  ...      0      0      0      0      0      0      0   \n",
       "4  0.303809  0.812646  ...      0      0      0      0      0      0      0   \n",
       "\n",
       "   D1774  D1775  D1776  \n",
       "0      0      0      0  \n",
       "1      0      1      0  \n",
       "2      0      0      0  \n",
       "3      0      0      0  \n",
       "4      0      0      0  \n",
       "\n",
       "[5 rows x 1777 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Загружаем данные\n",
    "data = pd.read_csv('data/_train_sem09 (1).csv')\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как предварительная обработка не требуется, пропускаем этот этап."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим на сбалансированность классов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data=data, x='Activity');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем матрицу наблюдений $X$ и вектор ответов $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['Activity'], axis=1)\n",
    "y = data['Activity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделяем выборку на тренировочную и тестовую в соотношении 80/20. Для сохранения соотношений целевого признака используем параметр stratify (стратифицированное разбиение). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Логистическая регрессия** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score на тренировочной выборке: 0.89\n",
      "f1_score на тестовой выборке: 0.78\n"
     ]
    }
   ],
   "source": [
    "#Создаем объект класса логистическая регрессия\n",
    "log_reg = linear_model.LogisticRegression(max_iter=500, random_state=42)\n",
    "\n",
    "#Обучаем модель\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "#Делаем предсказание f1-score на тренировочной и тестовой выборках\n",
    "y_train_pred = log_reg.predict(X_train)\n",
    "print('f1_score на тренировочной выборке: {:.2f}'.format(metrics.f1_score(y_train, y_train_pred)))\n",
    "y_test_pred = log_reg.predict(X_test)\n",
    "print('f1_score на тестовой выборке: {:.2f}'.format(metrics.f1_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchSV для логистической регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Импортируем библиотеку \n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:516: FitFailedWarning: \n",
      "70 fits failed out of a total of 280.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "19 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\base.py\", line 1356, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\base.py\", line 469, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'elasticnet', 'l2', 'l1'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "39 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\base.py\", line 1356, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\base.py\", line 469, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l1', 'l2', 'elasticnet'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\base.py\", line 1356, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\base.py\", line 469, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'elasticnet', 'l1', 'l2'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "6 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\base.py\", line 1356, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\base.py\", line 469, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'elasticnet', 'l1'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\base.py\", line 1356, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\base.py\", line 469, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l1', 'elasticnet', 'l2'} or None. Got 'none' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning: One or more of the test scores are non-finite: [0.763      0.762             nan        nan 0.76233333 0.76133333\n",
      "        nan        nan 0.754      0.75433333        nan        nan\n",
      " 0.75666667 0.757             nan        nan 0.753      0.75333333\n",
      "        nan        nan 0.75166667 0.75133333        nan        nan\n",
      " 0.75066667 0.75133333        nan        nan 0.743      0.743\n",
      " 0.76033333 0.76266667 0.76       0.75933333 0.76066667 0.76166667\n",
      " 0.76466667 0.76466667 0.754      0.75433333 0.764      0.763\n",
      " 0.75733333 0.75633333 0.76233333 0.76033333 0.75333333 0.753\n",
      " 0.75833333 0.75666667 0.75033333 0.752      0.75766667 0.754\n",
      " 0.75       0.75066667]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.11 s\n",
      "Wall time: 5min 23s\n",
      "f1_score на тестовом наборе: 0.78\n",
      "Наилучшие значения гиперпараметров: {'C': 0.3, 'penalty': 'l1', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "#Указываем искомые гиперпараметры в виде словаря \n",
    "param_grid = [\n",
    "    {'penalty': ['l2', 'none'], #тип регуляризации\n",
    "     'solver': ['lbfgs', 'sag'], #алгоритм оптимизации\n",
    "     'C': [0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 1]}, #уровень силы регуляризации\n",
    "    {'penalty': ['l1', 'l2'],\n",
    "     'solver': ['liblinear', 'saga'],\n",
    "     'C': [0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 1]}\n",
    "]\n",
    "\n",
    "#Вызываем класс GridSearchCV\n",
    "grid_search_log_reg = GridSearchCV(\n",
    "    estimator=log_reg,\n",
    "    param_grid=param_grid,\n",
    "    cv = 5,\n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "# %time - замер времени\n",
    "%time grid_search_log_reg.fit(X_train, y_train)\n",
    "\n",
    "#Делаем предсказание\n",
    "y_test_pred = grid_search_log_reg.predict(X_test)\n",
    "\n",
    "#Выводим наилучшую найденную комбинацию гиперпараметров и метрику\n",
    "print('f1_score на тестовом наборе: {:.2f}'.format(metrics.f1_score(y_test, y_test_pred)))\n",
    "print('Наилучшие значения гиперпараметров: {}'.format(grid_search_log_reg.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При помощи GridSearchCV не удалось улучшить целевую метрику на тествой выборке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomizedSearchCV для логистической регрессии\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Импортируем библиотеку\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:516: FitFailedWarning: \n",
      "40 fits failed out of a total of 250.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "11 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\base.py\", line 1356, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\base.py\", line 469, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'elasticnet', 'l1', 'l2'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\base.py\", line 1356, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\base.py\", line 469, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l1', 'l2', 'elasticnet'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "18 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\base.py\", line 1356, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\base.py\", line 469, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l1', 'elasticnet', 'l2'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\base.py\", line 1356, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\base.py\", line 469, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'elasticnet', 'l1'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\base.py\", line 1356, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\base.py\", line 469, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'elasticnet', 'l2', 'l1'} or None. Got 'none' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning: One or more of the test scores are non-finite: [0.75066667 0.756      0.75       0.75266667 0.743      0.755\n",
      " 0.763      0.764      0.754      0.75666667 0.76266667 0.76433333\n",
      " 0.75066667 0.76033333 0.75566667        nan        nan 0.75233333\n",
      " 0.75733333        nan 0.754      0.76566667 0.75333333 0.76533333\n",
      " 0.757      0.76366667        nan 0.766      0.75033333 0.75433333\n",
      " 0.75433333        nan        nan 0.75133333 0.75766667 0.76066667\n",
      " 0.75766667 0.75566667 0.76166667 0.76166667        nan 0.756\n",
      " 0.752      0.75666667        nan 0.763      0.75666667 0.75066667\n",
      " 0.76266667 0.75666667]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 39.6 s\n",
      "Wall time: 5min 46s\n",
      "f1_score на тестовом наборе: 0.78\n",
      "Наилучшие значения гиперпараметров: {'solver': 'saga', 'penalty': 'l1', 'C': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Указываем искомые гиперпараметры в виде словаря \n",
    "param_distributions = [{'penalty': ['l2', 'none'],\n",
    "                       'solver': ['lbfgs', 'sag'],\n",
    "                       'C': list(np.linspace(0.01, 1, 10, dtype=float))},\n",
    "                       {'penalty': ['l1', 'l2'],\n",
    "                        'solver': ['liblinear', 'saga'],\n",
    "                        'C': list(np.linspace(0.01, 1, 10, dtype=float))}\n",
    "                       ]\n",
    "\n",
    "#Вызываем класс RandomizedSearchCV\n",
    "random_search_log_reg = RandomizedSearchCV(\n",
    "    estimator=log_reg,\n",
    "    param_distributions=param_distributions,\n",
    "    cv = 5,\n",
    "    n_iter = 50,\n",
    "    n_jobs = -1 \n",
    ")\n",
    "\n",
    "%time random_search_log_reg.fit(X_train, y_train)\n",
    "\n",
    "#Делаем предсказание\n",
    "y_test_pred = random_search_log_reg.predict(X_test)\n",
    "\n",
    "#Выводим наилучшую найденную комбинацию гиперпараметров и метрику\n",
    "print('f1_score на тестовом наборе: {:.2f}'.format(metrics.f1_score(y_test, y_test_pred)))\n",
    "print('Наилучшие значения гиперпараметров: {}'.format(random_search_log_reg.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При помощи RandomizedSearchCV не удалось улучшить целевую метрику на тествой выборке. Но данный метод отработал быстрее, чем GridSearchCV. (Но не всегда быстрее отрабатывает)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HYPEROPT для логистической регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Импортируем библиотеки\n",
    "import hyperopt\n",
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "  0%|          | 0/25 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1224: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/25 [00:02<00:57,  2.39s/trial, best loss: -0.7515816136471943]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [00:10<02:06,  5.50s/trial, best loss: -0.7802155861598277]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3/25 [00:12<01:28,  4.02s/trial, best loss: -0.7880614801082855]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 4/25 [00:20<01:54,  5.46s/trial, best loss: -0.7880614801082855]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1224: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20%|██        | 5/25 [00:22<01:27,  4.35s/trial, best loss: -0.7880614801082855]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1224: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6/25 [00:30<01:47,  5.65s/trial, best loss: -0.7880614801082855]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1224: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 7/25 [00:38<01:55,  6.43s/trial, best loss: -0.7880614801082855]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 8/25 [00:41<01:29,  5.24s/trial, best loss: -0.7880614801082855]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1224: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 9/25 [00:43<01:10,  4.38s/trial, best loss: -0.7880614801082855]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 11/25 [00:54<01:03,  4.55s/trial, best loss: -0.7898377171380607]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1224: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 12/25 [01:03<01:16,  5.86s/trial, best loss: -0.7898377171380607]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 13/25 [01:05<00:57,  4.78s/trial, best loss: -0.7898377171380607]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 14/25 [01:13<01:03,  5.77s/trial, best loss: -0.7898377171380607]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 15/25 [01:21<01:03,  6.40s/trial, best loss: -0.7898377171380607]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1224: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 16/25 [01:23<00:46,  5.22s/trial, best loss: -0.7898377171380607]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1224: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 17/25 [01:32<00:49,  6.23s/trial, best loss: -0.7898377171380607]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 18/25 [01:40<00:47,  6.80s/trial, best loss: -0.7898377171380607]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 19/25 [01:48<00:42,  7.13s/trial, best loss: -0.7898377171380607]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 20/25 [01:50<00:28,  5.72s/trial, best loss: -0.7898377171380607]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 21/25 [01:53<00:19,  4.79s/trial, best loss: -0.7898377171380607]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 23/25 [01:58<00:06,  3.49s/trial, best loss: -0.794911910450625] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [02:02<00:00,  4.91s/trial, best loss: -0.794911910450625]\n",
      "Наилучшие значения гиперпараметров {'C': 0.017331323696411022, 'penalty': 0, 'solver': 0}\n"
     ]
    }
   ],
   "source": [
    "#Указываем искомые гиперпараметры в виде словаря \n",
    "space = {'penalty': hp.choice('penalty', ['l2', None]),\n",
    "          'solver': hp.choice('solver', ['lbfgs', 'sag']),\n",
    "          'C': hp.uniform('C', 0.01, 1)}\n",
    "\n",
    "#Зафиксируем random_state\n",
    "random_state = 42\n",
    "#Пишем функцию hyperopt\n",
    "def hyperopt_log_reg(params, cv=5, X=X_train,\n",
    "                     y=y_train, random_state=random_state):\n",
    "    #Функция получает комбинацию гиперпараметов в params\n",
    "    params = {'penalty': params['penalty'],\n",
    "              'solver': params['solver'],\n",
    "              'C': float(params['C'])\n",
    "              }\n",
    "    \n",
    "    #Используем эту комбинацию для построения модели\n",
    "    model = linear_model.LogisticRegression(**params, random_state=random_state)\n",
    "\n",
    "    #Обучим модель с помощью кросс-валидации\n",
    "    model.fit(X, y)\n",
    "    score = cross_val_score(model, X, y, cv=cv, scoring='f1', n_jobs=-1).mean()\n",
    "\n",
    "    return -score\n",
    "\n",
    "%time\n",
    "#Начинаем подбор гиперпараметров\n",
    "\n",
    "#Используется для логирования результатов\n",
    "trials = Trials()\n",
    "\n",
    "best = fmin(hyperopt_log_reg,#наша функция\n",
    "            space=space,#пространство гиперпараметров\n",
    "            algo=tpe.suggest,#алгоритм оптимизации, \n",
    "            #установлен по умолчанию, задавать необязательно\n",
    "            max_evals=25,#максимальное количество итераций\n",
    "            trials=trials,#логирование результатов\n",
    "            rstate=np.random.default_rng(random_state)\n",
    "            #фиксируем для повторяемости результата\n",
    "            )\n",
    "\n",
    "print('Наилучшие значения гиперпараметров {}'.format(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score на тестовом наборе: 0.78\n"
     ]
    }
   ],
   "source": [
    "#Преобразуем best в параметры модели\n",
    "penalty_options = ['l2', None]\n",
    "solver_options = ['lbfgs', 'sag']\n",
    "\n",
    "best_params = {\n",
    "    'penalty':penalty_options[best['penalty']],\n",
    "    'solver':solver_options[best['solver']],\n",
    "    'C':float(best['C'])\n",
    "}\n",
    "\n",
    "#Рассчитам точность для тестовой выборки\n",
    "model_hyperopt_log_reg = linear_model.LogisticRegression(\n",
    "    random_state=random_state,\n",
    "    **best_params\n",
    "    )\n",
    "\n",
    "model_hyperopt_log_reg.fit(X_train, y_train)\n",
    "\n",
    "#Делаем предсказание\n",
    "y_test_pred = model_hyperopt_log_reg.predict(X_test)\n",
    "\n",
    "#Выводим наилучшую найденную комбинацию гиперпараметров и метрику\n",
    "print('f1_score на тестовом наборе: {:.2f}'.format(metrics.f1_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На подборе гиперпараметров можно наблюдать значения -0.7949..., что говорит о том, что значение f1_score должно быть 0.79 - 0.8. Но после обучения модели с наилучшими гиперпараметрами метрика осталась неизменной. Но данный метод, подобрал гиперпараметры быстрее, чем два предыдущих."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna для логистической регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Импортируем библиотеку\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Настроим пространства поиска гиперпараметров\n",
    "def optuna_log_reg(trial):\n",
    "    #Задаем пространства поиска гиперпараметров\n",
    "    penalty = trial.suggest_categorical('penalty', ['l2', None])\n",
    "    solver = trial.suggest_categorical('solver', ['lbfgs', 'sag'])\n",
    "    C = trial.suggest_float('C', 0.01, 1)\n",
    "\n",
    "    #Создаем модель\n",
    "    model = linear_model.LogisticRegression(penalty=penalty,\n",
    "                                            solver=solver,\n",
    "                                            C=C,\n",
    "                                            random_state=random_state)\n",
    "    \n",
    "    #Обучаем модель\n",
    "    model.fit(X_train, y_train)\n",
    "    score = metrics.f1_score(y_train, model.predict(X_train))\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-31 19:27:42,598] A new study created in memory with name: LogisticRegression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-31 19:27:43,248] Trial 0 finished with value: 0.8849772382397572 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.5730948043317812}. Best is trial 0 with value: 0.8849772382397572.\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1224: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-10-31 19:27:46,821] Trial 1 finished with value: 0.8928679817905918 and parameters: {'penalty': None, 'solver': 'sag', 'C': 0.9829663299140792}. Best is trial 1 with value: 0.8928679817905918.\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1224: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-31 19:27:47,406] Trial 2 finished with value: 0.912883435582822 and parameters: {'penalty': None, 'solver': 'lbfgs', 'C': 0.7626773974941944}. Best is trial 2 with value: 0.912883435582822.\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1224: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-31 19:27:47,983] Trial 3 finished with value: 0.912883435582822 and parameters: {'penalty': None, 'solver': 'lbfgs', 'C': 0.3363899970160763}. Best is trial 2 with value: 0.912883435582822.\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1224: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-10-31 19:27:51,572] Trial 4 finished with value: 0.8928679817905918 and parameters: {'penalty': None, 'solver': 'sag', 'C': 0.36221976863687805}. Best is trial 2 with value: 0.912883435582822.\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1224: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-31 19:27:52,172] Trial 5 finished with value: 0.912883435582822 and parameters: {'penalty': None, 'solver': 'lbfgs', 'C': 0.9004992973749715}. Best is trial 2 with value: 0.912883435582822.\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1224: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-31 19:27:52,765] Trial 6 finished with value: 0.912883435582822 and parameters: {'penalty': None, 'solver': 'lbfgs', 'C': 0.4754698577986078}. Best is trial 2 with value: 0.912883435582822.\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1224: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-10-31 19:27:56,337] Trial 7 finished with value: 0.8928679817905918 and parameters: {'penalty': None, 'solver': 'sag', 'C': 0.37702706750556353}. Best is trial 2 with value: 0.912883435582822.\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1224: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-10-31 19:27:59,912] Trial 8 finished with value: 0.8928679817905918 and parameters: {'penalty': None, 'solver': 'sag', 'C': 0.686610271457181}. Best is trial 2 with value: 0.912883435582822.\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-10-31 19:28:03,500] Trial 9 finished with value: 0.8791742562234366 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.44200888000564503}. Best is trial 2 with value: 0.912883435582822.\n",
      "[I 2025-10-31 19:28:04,053] Trial 10 finished with value: 0.8416965352449224 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.049071972449868495}. Best is trial 2 with value: 0.912883435582822.\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1224: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-31 19:28:04,657] Trial 11 finished with value: 0.912883435582822 and parameters: {'penalty': None, 'solver': 'lbfgs', 'C': 0.14505741621848542}. Best is trial 2 with value: 0.912883435582822.\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1224: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-31 19:28:05,247] Trial 12 finished with value: 0.912883435582822 and parameters: {'penalty': None, 'solver': 'lbfgs', 'C': 0.7493778532053376}. Best is trial 2 with value: 0.912883435582822.\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1224: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-31 19:28:05,829] Trial 13 finished with value: 0.912883435582822 and parameters: {'penalty': None, 'solver': 'lbfgs', 'C': 0.2669521423755552}. Best is trial 2 with value: 0.912883435582822.\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-31 19:28:06,413] Trial 14 finished with value: 0.8872043662825955 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.6631069681457958}. Best is trial 2 with value: 0.912883435582822.\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1224: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-31 19:28:07,065] Trial 15 finished with value: 0.912883435582822 and parameters: {'penalty': None, 'solver': 'lbfgs', 'C': 0.8667276571677438}. Best is trial 2 with value: 0.912883435582822.\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1224: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-31 19:28:07,721] Trial 16 finished with value: 0.912883435582822 and parameters: {'penalty': None, 'solver': 'lbfgs', 'C': 0.24112834372903463}. Best is trial 2 with value: 0.912883435582822.\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1224: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-31 19:28:08,329] Trial 17 finished with value: 0.912883435582822 and parameters: {'penalty': None, 'solver': 'lbfgs', 'C': 0.5765617513101651}. Best is trial 2 with value: 0.912883435582822.\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-31 19:28:08,924] Trial 18 finished with value: 0.8906392002423508 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.8018078413584409}. Best is trial 2 with value: 0.912883435582822.\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1224: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-10-31 19:28:09,533] Trial 19 finished with value: 0.912883435582822 and parameters: {'penalty': None, 'solver': 'lbfgs', 'C': 0.6156867362018932}. Best is trial 2 with value: 0.912883435582822.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 25s\n",
      "Wall time: 26.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Создаем объект исследования\n",
    "#Можем напрямую указать, что нам необходимо максимизировать метрику\n",
    "#direction='maximize'\n",
    "study = optuna.create_study(study_name='LogisticRegression', direction='maximize')\n",
    "#Ищем лучшую комбинацию гиперпараметров n_trials раз\n",
    "study.optimize(optuna_log_reg, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наилучшие значения гиперпараметров {'penalty': None, 'solver': 'lbfgs', 'C': 0.7626773974941944}\n",
      "f1_score на обучающем наборе: 0.91\n"
     ]
    }
   ],
   "source": [
    "#Выводим результаты на обучающей выборке\n",
    "print('Наилучшие значения гиперпараметров {}'.format(study.best_params))\n",
    "print('f1_score на обучающем наборе: {:.2f}'.format(study.best_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1224: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score на тестовом наборе: 0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files (x86)\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#Рассчитаем точность для тестовой выборки\n",
    "model_optuna_log_reg = linear_model.LogisticRegression(**study.best_params, random_state=random_state)\n",
    "\n",
    "model_optuna_log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = model_optuna_log_reg.predict(X_test)\n",
    "print('f1_score на тестовом наборе: {:.2f}'.format(metrics.f1_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К сожалению, Optuna показала ухудшение результата. Но подбор гиперпараметров отработал быстрее всего."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Случайный лес** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score на тренировочной выборке: 1.00\n",
      "f1_score на тестовой выборке: 0.81\n"
     ]
    }
   ],
   "source": [
    "#Создаем объект класса случайный лес\n",
    "rf = ensemble.RandomForestClassifier(random_state=42)\n",
    "\n",
    "#Обучаем модель\n",
    "rf.fit(X_train, y_train)\n",
    "#Выводим значения метрики\n",
    "y_train_pred = rf.predict(X_train)\n",
    "print('f1_score на тренировочной выборке: {:.2f}'.format(metrics.f1_score(y_train, y_train_pred)))\n",
    "y_test_pred = rf.predict(X_test)\n",
    "print('f1_score на тестовой выборке: {:.2f}'.format(metrics.f1_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchSV для случайного леса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.61 s\n",
      "Wall time: 1min 10s\n",
      "f1_score на тестовом наборе: 0.81\n",
      "Наилучшие значения гиперпараметров: {'max_depth': 20, 'min_samples_leaf': 5, 'n_estimators': 170}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'n_estimators': list(range(80, 200, 30)),\n",
    "              'min_samples_leaf': [5],\n",
    "              'max_depth': list(np.linspace(20, 40, 10, dtype=int))\n",
    "              }\n",
    "\n",
    "grid_search_rf = GridSearchCV(\n",
    "    estimator=ensemble.RandomForestClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=5, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "%time grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = grid_search_rf.predict(X_test)\n",
    "\n",
    "print('f1_score на тестовом наборе: {:.2f}'.format(metrics.f1_score(y_test, y_test_pred)))\n",
    "print('Наилучшие значения гиперпараметров: {}'.format(grid_search_rf.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
